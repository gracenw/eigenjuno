{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, json, math, pickle\n",
    "import spiceypy as spice\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from utils import classify, contrast_resize\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for figure readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.family': 'Serif'})\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.expanduser(\"~/eigenjuno/DATA/TRAIN/\")\n",
    "TEST_DIR = os.path.expanduser(\"~/eigenjuno/DATA/TEST/\")\n",
    "KERNEL_DIR = os.path.expanduser(\"~/eigenjuno/STITCHING/KERNELS/CURRENT/\")\n",
    "VEC_SIZE = 256 * 256 * 3\n",
    "VEC_DIM = (256, 256, 3)\n",
    "TRAIN_SIZE = 400\n",
    "TEST_NAME = '6743'\n",
    "TEST_IMGS = [6743] #, 6745, 6746, 6749, 6750, 6582, 6587, 6978, 6980, 6983, 6984, 6991, 6993, 6994, 10348, 5192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = np.empty(TRAIN_SIZE)\n",
    "data = np.empty((TRAIN_SIZE, VEC_SIZE))\n",
    "\n",
    "for r, d, f in os.walk(TRAIN_DIR + 'ONE/'):\n",
    "    for file in f:\n",
    "        num = int(file.split('.')[0])\n",
    "        tags[num - 1] = 1\n",
    "        img = mpimg.imread(os.path.join(TRAIN_DIR, 'ONE/' + str(num) + '.png'))\n",
    "        data[num - 1, :] = img[:, :, :3].reshape(VEC_SIZE)\n",
    "        \n",
    "for r, d, f in os.walk(TRAIN_DIR + 'ZERO/'):\n",
    "    for file in f:\n",
    "        num = int(file.split('.')[0])\n",
    "        tags[num - 1] = 0\n",
    "        img = mpimg.imread(os.path.join(TRAIN_DIR, 'ZERO/' + str(num) + '.png'))\n",
    "        data[num - 1, :] = img[:, :, :3].reshape(VEC_SIZE)\n",
    "\n",
    "tags = tags.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform training w/ pca and eigenimaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, tags, test_size = 0.2)\n",
    "\n",
    "# perform pca transform\n",
    "pca = PCA(n_components = 0.9, svd_solver = 'full').fit(X_train)\n",
    "print('Decomposed test set to', pca.components_.shape[0], 'components')\n",
    "X_train_PCA = pca.transform(X_train)=\n",
    "\n",
    "# apply pca transform to x_test\n",
    "X_test_PCA = pca.transform(X_test)\n",
    "\n",
    "# fit classifier to data\n",
    "C_RANGE = np.logspace(-3, 3, 20)\n",
    "G_RANGE = np.logspace(-9, 1, 20)\n",
    "param_grid = { 'C': C_RANGE,\n",
    "               'gamma': ['scale', 'auto'],\n",
    "               'gamma': G_RANGE,\n",
    "               'degree': [2, 3, 4],\n",
    "               'kernel': ['rbf', 'poly'], \n",
    "               'coef0': [0, 1],\n",
    "               'class_weight': ['balanced'] }\n",
    "\n",
    "svm = GridSearchCV(SVC(), param_grid, n_jobs = -1, verbose = 0)\n",
    "svm.fit(X_train_PCA, y_train)\n",
    "\n",
    "Copt = svm.best_params_['C'] # svm cost parameter\n",
    "Kopt = svm.best_params_['kernel'] # kernel function\n",
    "Gopt = svm.best_params_['gamma'] # gamma of RBF kernel\n",
    "Dopt = svm.best_params_['degree'] # degree of polynomial kernel\n",
    "Zopt = svm.best_params_['coef0'] # independent term in poly kernel\n",
    "\n",
    "print('\\nOptimal SVM parameter values:')\n",
    "print('C:', Copt)\n",
    "print('kernel:', Kopt)\n",
    "print('gamma:', Gopt)\n",
    "print('degree:', Dopt)\n",
    "print('coef0:', Zopt, '\\n')\n",
    "\n",
    "# generate report\n",
    "print('Calculating metrics...')\n",
    "y_pred = svm.predict(X_test_PCA)\n",
    "print(classification_report(y_test, y_pred))\n",
    "scores = cross_val_score(svm, X_test_PCA, y_test, cv = 6)\n",
    "print('\\nAverage cross-validate score: ', scores.mean())\n",
    "\n",
    "# dump model data for safe-keeping\n",
    "pickle.dump(svm, open('MODELS/svm_model_' + datetime.now().strftime(\"%d:%m:%Y_%H:%M:%S\") + '.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divide-(and shift)-and-conquer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in TEST_IMGS:\n",
    "    detections, fig = classify(os.path.join(TEST_DIR, str(img) + '-Stitched.png'))\n",
    "    fig.show()\n",
    "#     fig.savefig('FIGURES/detection_maps_' + str(test_name) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save optimal model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svm, open('MODELS/svm_model_opt.sav', 'wb'))\n",
    "pickle.dump(pca, open('MODELS/pca_model_opt.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load optimal model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = pickle.load(open('MODELS/svm_model_opt.sav', 'rb'))\n",
    "pca = pickle.load(open('MODELS/pca_model_opt.sav', 'rb'))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, tags, test_size = 0.2)\n",
    "X_test_PCA = pca.transform(X_test)\n",
    "y_pred = svm.predict(X_test_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contrast and resize new training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, d, f in os.walk(TRAIN_DIR + 'PROCESS/'):\n",
    "    for file in f:\n",
    "        num = int(file.split('.')[0])\n",
    "        img = contrast_resize(os.path.join(TRAIN_DIR, 'PROCESS/' + str(num) + '.png'), (256, 256))\n",
    "        plt.imsave(os.path.join(TRAIN_DIR, 'ONE/' + str(num) + '.png'), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display contrasted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(7, 7, figsize = (15, 20));\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(data[i, :].reshape(VEC_DIM))\n",
    "    ax.title.set_text('VALUE ' + str(int(tags[i])))\n",
    "    ax.title.set_fontsize(14)\n",
    "    ax.axis('off')\n",
    "fig.savefig('FIGURES/display_dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize results w/ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 6, figsize = (15, 20));\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test[i - 1, :].reshape(VEC_DIM))\n",
    "    ax.title.set_text('VALUE ' + str(int(y_test[i - 1])) + \"\\n PREDICTED \" + str(int(y_pred[i - 1])))\n",
    "    ax.title.set_fontsize(14)\n",
    "    ax.axis('off')\n",
    "plt.savefig('FIGURES/sample_outputs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca with uncontrasted data - fewer components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_MIN = PCA(n_components = pca.components_.shape[0]).fit(data)\n",
    "plt.figure(figsize = (10, 7));\n",
    "plt.plot(pca_MIN.explained_variance_ratio_.cumsum(), color = 'seagreen');\n",
    "plt.xlabel('Number of Principal Components');\n",
    "plt.ylabel('Explained Variance Ratio');\n",
    "plt.title('Explained Variance Ratio of\\nPrincipal Components in a Contrasted Dataset');\n",
    "plt.savefig('FIGURES/pca_variance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_indices = svm.support_\n",
    "fig, axes = plt.subplots(5, 4, figsize = (18, 22))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < sv_indices.size:\n",
    "        ax.imshow(pca.inverse_transform(svm.support_vectors_[i, :]).reshape((VEC_DIM)))\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "fig.savefig('FIGURES/support_vectors.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize eigenfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 4, figsize = (18, 25))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < pca.components_.shape[0]:\n",
    "        ax.title.set_text(str(pca.singular_values_[i]))\n",
    "        ax.imshow(pca.components_[i, :].reshape((VEC_DIM)) * 255)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "fig.savefig('FIGURES/eigenfaces.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance of training space parameter vs num support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G_RANGE = np.logspace(-9, 1, 20)\n",
    "\n",
    "NUM_SVS = []\n",
    "\n",
    "data_PCA = pca.transform(data)\n",
    "\n",
    "for G in G_RANGE:\n",
    "    svm = SVC(C = 1, kernel = 'rbf', gamma = G, class_weight = 'balanced')\n",
    "    svm.fit(data_PCA, tags)\n",
    "    NUM_SVS.append(svm.support_vectors_.shape[0])\n",
    "    \n",
    "plt.figure(figsize = (10, 7));\n",
    "plt.plot(C_RANGE, NUM_SVS, color = 'lightcoral');\n",
    "plt.xscale('log');\n",
    "plt.xlabel('G');\n",
    "plt.ylabel('Support Vectors');\n",
    "plt.title('Gamma Value (Variance of Training Space)\\nvs. Number of Support Vectors')\n",
    "plt.savefig('FIGURES/gamma_sv_tradeoff.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma vs number of support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_RANGE = np.logspace(-2, 4, 20)\n",
    "\n",
    "NUM_SVS = []\n",
    "\n",
    "data_PCA = pca.transform(data)\n",
    "\n",
    "for C in C_RANGE:\n",
    "    svm = SVC(C = C, kernel = 'rbf', gamma = 'scale', class_weight = 'balanced')\n",
    "    svm.fit(data_PCA, tags)\n",
    "    NUM_SVS.append(svm.support_vectors_.shape[0])\n",
    "    \n",
    "plt.figure(figsize = (10, 7));\n",
    "plt.plot(C_RANGE, NUM_SVS, color = 'darkorchid');\n",
    "plt.xscale('log');\n",
    "plt.xlabel('C');\n",
    "plt.ylabel('Support Vectors');\n",
    "plt.title('Cost Value (L2 Regularization Parameter)\\nvs. Number of Support Vectors')\n",
    "plt.savefig('FIGURES/cost_sv_tradeoff.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple pca example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 6))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection = '3d')\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "n = 100\n",
    "\n",
    "xyz = np.empty((n, 3))\n",
    "\n",
    "## generate random 3D data\n",
    "xyz[:, 0] = np.linspace(0, 1, 100)\n",
    "xyz[:, 1] = np.sin(9 * xyz[:, 0]) + np.sqrt(1 / 3.0) * np.random.randn(n)\n",
    "xyz[:, 2] = np.random.rand(n)\n",
    "\n",
    "## define binary classes - green triangle, pink circle\n",
    "for i in range(xyz.shape[0]):\n",
    "    if xyz[i, 1] < 0.5:\n",
    "        ax1.scatter(xyz[i, 0], xyz[i, 1], xyz[i, 2], marker = '^', color = 'green')\n",
    "    else:\n",
    "        ax1.scatter(xyz[i, 0], xyz[i, 1], xyz[i, 2], marker = 'o', color = 'hotpink')\n",
    "\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "ax1.set_title('Data before PCA (3D)')\n",
    "\n",
    "for label in (ax1.get_xticklabels() + ax1.get_yticklabels() + ax1.get_zticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "\n",
    "## reduce dimension from 3D to 2D\n",
    "pca_xyz = PCA(n_components = 2).fit(xyz)\n",
    "xyz_d = pca_xyz.transform(xyz)\n",
    "\n",
    "for i in range(xyz.shape[0]):\n",
    "    if xyz[i, 1] < 0.5:\n",
    "        ax2.scatter(xyz_d[i, 0], xyz_d[i, 1], marker = '^', color = 'green')\n",
    "    else:\n",
    "        ax2.scatter(xyz_d[i, 0], xyz_d[i, 1], marker = 'o', color = 'hotpink')\n",
    "\n",
    "ax2.set_xlabel('PC 1')\n",
    "ax2.set_ylabel('PC 2')\n",
    "ax2.set_title('Data after PCA (2D)')\n",
    "\n",
    "for label in (ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "\n",
    "fig.savefig('FIGURES/pca_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding coordinates with spice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the kernels\n",
    "KERNELS = []\n",
    "for r, d, f in os.walk(KERNEL_DIR):\n",
    "    for file in f:\n",
    "        KERNELS.append(file)\n",
    "\n",
    "spice.kclear()\n",
    "for k in KERNELS:\n",
    "    spice.furnsh(os.path.join(KERNEL_DIR, k))\n",
    "\n",
    "## read image metadata\n",
    "with open(os.path.join(TEST_DIR, TEST_NAME + '-Metadata.json'), 'r') as f:\n",
    "    img_json = json.load(f)\n",
    "    image = img_json['FILE_NAME']\n",
    "    image_time = img_json['START_TIME']\n",
    "et = spice.str2et(image_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find lat/long of camera location of jupiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get position of juno relative to jupiter at elapsed time\n",
    "pos, orient = spice.spkpos('JUNO', et, 'IAU_JUPITER', 'NONE', 'JUPITER')\n",
    "\n",
    "## rotation matrix from juno spacecraft to 'cube' modeling camera - from juno_v12.tf\n",
    "JUNO_TO_CUBE = np.matrix([[-0.0059163, -0.0142817, -0.9998805], \n",
    "                          [ 0.0023828, -0.9998954,  0.0142678], \n",
    "                          [-0.9999797, -0.0022981,  0.0059497]])\n",
    "\n",
    "## rotation matrix from 'cube' to camera (aberration correction) - from juno_v12.tf\n",
    "CUBE_TO_CAM = (R.from_euler('zyx', [0.69, -0.469,  0.583])).as_matrix()\n",
    "\n",
    "## combine rotation matrices - application order JUNO_TO_CUBE then CUBE_TO_CAM\n",
    "rot_matrix = CUBE_TO_CAM * JUNO_TO_CUBE\n",
    "\n",
    "## apply rotations to juno position to get position camera points to\n",
    "pos = rot_matrix * pos.reshape(-1, 1)\n",
    "\n",
    "## calculate planetocentric lat/long coordinates in radians\n",
    "_, long, lat = spice.reclat(np.ravel(pos))\n",
    "\n",
    "## convert coordinates to degrees\n",
    "long, lat = np.array([long, lat]) * 180 / math.pi\n",
    "\n",
    "print(long, lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert surface raster to lat/long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = np.load(os.path.join(TEST_DIR, TEST_NAME + '-Raster.npy'))\n",
    "coords = np.empty((2048, 2048, 2))\n",
    "for i in range(2048):\n",
    "    for j in range(2048):\n",
    "        if abs(np.mean(raster[i, j, :])) != 0:\n",
    "            _, long, lat = spice.reclat(raster[i, j, :])\n",
    "            coords[i, j, :] = np.array([long, lat]) * 180 / math.pi\n",
    "        else:\n",
    "            coords[i, j, :] = np.array([np.nan, np.nan]) # outside range\n",
    "np.save(os.path.join(TEST_DIR, TEST_NAME + '-Coords', coords))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "ax2.imshow(raster)\n",
    "ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save detections in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_indices = np.argwhere(threshold == 1)\n",
    "det_coords = np.empty((det_indices.shape[0], 2))\n",
    "for i in range(det_indices.shape[0]):\n",
    "    det_coords[i, :] = coords[det_indices[i, 0], det_indices[i, 1]]\n",
    "np.save('LOGS/' + TEST_NAME + '_' + image_time, det_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrain model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ** detections should be certified to be correct by human user ** ##\n",
    "fig, axes = plt.subplots(6, 6, figsize = (18, 20))\n",
    "green_indices = []\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    block = retrain_blocks[i, :].reshape((VEC_DIM)) * 255\n",
    "    if len(np.where((block[:, :, 0] == 0) & (block[:, :, 1] != 0) & (block[:, :, 2] != 0))[0]) == 0:\n",
    "        ax.imshow(block / 255)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "        green_indices.append(i)\n",
    "retrain_blocks = np.delete(retrain_blocks, green_indices, axis = 0)\n",
    "\n",
    "curr = TRAIN_SIZE + 1\n",
    "for i in range(retrain_blocks.shape[0]):\n",
    "    plt.imsave(str(curr) + '.png', retrain_blocks[i, :].reshape((VEC_DIM)))\n",
    "    curr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
