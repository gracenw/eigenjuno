{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, json, math, pickle, fnmatch\n",
    "import spiceypy as spice\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from utils import contrast_resize\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for figure readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.family': 'Serif'})\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.expanduser(\"~/eigenjuno/DATA/TRAIN/\")\n",
    "TEST_DIR = os.path.expanduser(\"~/eigenjuno/DATA/TEST/\")\n",
    "KERNEL_DIR = os.path.expanduser(\"~/eigenjuno/STITCHING/KERNELS/CURRENT/\")\n",
    "VEC_SIZE = 256 * 256 * 3\n",
    "VEC_DIM = (256, 256, 3)\n",
    "TRAIN_SIZE = 400\n",
    "TEST_NAME = '6745'\n",
    "TEST_IMGS = [6745] # , 6743, 6746, 6749, 6750, 6582, 6587, 6978, 6980, 6983, 6984, 6991, 6993, 6994, 10348, 5192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = np.empty(TRAIN_SIZE)\n",
    "data = np.empty((TRAIN_SIZE, VEC_SIZE))\n",
    "\n",
    "for r, d, f in os.walk(TRAIN_DIR + 'ONE/'):\n",
    "    for file in f:\n",
    "        num = int(file.split('.')[0])\n",
    "        tags[num - 1] = 1\n",
    "        img = mpimg.imread(os.path.join(TRAIN_DIR, 'ONE/' + str(num) + '.png'))\n",
    "        data[num - 1, :] = img[:, :, :3].reshape(VEC_SIZE)\n",
    "        \n",
    "for r, d, f in os.walk(TRAIN_DIR + 'ZERO/'):\n",
    "    for file in f:\n",
    "        num = int(file.split('.')[0])\n",
    "        tags[num - 1] = 0\n",
    "        img = mpimg.imread(os.path.join(TRAIN_DIR, 'ZERO/' + str(num) + '.png'))\n",
    "        data[num - 1, :] = img[:, :, :3].reshape(VEC_SIZE)\n",
    "\n",
    "tags = tags.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divide-(and shift)-and-conquer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_blocks(img):\n",
    "    \"\"\"\n",
    "    recursively shift, divide, and classify an image\n",
    "    :param img: 2048x2048 cv2 image\n",
    "    :return: detections, nd-array shape 2048x2048 - contains accumulated binary labels at each pixel\n",
    "    \"\"\"\n",
    "    \n",
    "    full_size = img.shape[0]\n",
    "    block_size = int(full_size / 2)\n",
    "    if block_size > 128:\n",
    "        shift_size = int(block_size / 2)\n",
    "        detections = np.zeros((full_size, full_size))\n",
    "        shift_img = np.copy(img)\n",
    "        \n",
    "        for i in range(1, 5):\n",
    "            temp = np.copy(shift_img)\n",
    "            shift_img[0:(full_size - shift_size), :, :] = np.copy(temp[shift_size:full_size, :, :])\n",
    "            shift_img[(full_size - shift_size):full_size, :, :] = np.copy(temp[0:shift_size, :, :])\n",
    "            \n",
    "            blocks = np.empty((4, block_size, block_size, 3))\n",
    "            blocks[0, :, :, :] = shift_img[0 : block_size, 0 : block_size, :]\n",
    "            blocks[1, :, :, :] = shift_img[0 : block_size, block_size : full_size, :]\n",
    "            blocks[2, :, :, :] = shift_img[block_size : full_size, 0 : block_size, :]\n",
    "            blocks[3, :, :, :] = shift_img[block_size : full_size, block_size : full_size, :]\n",
    "\n",
    "            valid_indices = []\n",
    "            testing_blocks = np.empty((4, VEC_SIZE))\n",
    "            for i in range(4):\n",
    "                block = cv2.resize(blocks[i, :, :], (256, 256), interpolation = cv2.INTER_NEAREST)\n",
    "                green_locs = np.where((block[:, :, 0] == 0) & (block[:, :, 1] != 0) & (block[:, :, 2] != 0))[0]\n",
    "                block = np.reshape(block, VEC_SIZE)\n",
    "                if (block.mean() > 15) and (green_locs.size < block.size / 3):\n",
    "                    valid_indices = np.append(valid_indices, i)\n",
    "                    testing_blocks[i, :] = block\n",
    "\n",
    "            current_dets = np.zeros(4)\n",
    "\n",
    "            if len(valid_indices) != 0:\n",
    "                testing_blocks = testing_blocks[np.array(valid_indices).astype(int), :]\n",
    "                transform_test = pca.transform(testing_blocks)\n",
    "                split_pred = svm.predict(transform_test)\n",
    "                index = 0\n",
    "                for i in range(4):\n",
    "                    if i in valid_indices:\n",
    "                        current_dets[i] = split_pred[index]\n",
    "                        index += 1\n",
    "\n",
    "            block_detections = np.zeros((full_size, full_size))\n",
    "            \n",
    "            block_detections[0 : block_size, 0 : block_size] = np.full((block_size, block_size), current_dets[0]) \\\n",
    "                        + break_blocks(blocks[0, :, :, :])\n",
    "            block_detections[0 : block_size, block_size : full_size] = np.full((block_size, block_size), current_dets[1]) \\\n",
    "                        + break_blocks(blocks[1, :, :, :])\n",
    "            block_detections[block_size : full_size, 0 : block_size] = np.full((block_size, block_size), current_dets[2]) \\\n",
    "                        + break_blocks(blocks[2, :, :, :])\n",
    "            block_detections[block_size : full_size, block_size : full_size] = np.full((block_size, block_size), current_dets[3]) \\\n",
    "                        + break_blocks(blocks[3, :, :, :])\n",
    "            \n",
    "            unshift_dets = np.empty((full_size, full_size))\n",
    "            unshift_dets[0:(i * shift_size), :] = np.copy(block_detections[(full_size - (shift_size * i)):full_size, :])\n",
    "            unshift_dets[(i * shift_size):full_size, :] = np.copy(block_detections[0:(full_size - (shift_size * i)), :])\n",
    "            \n",
    "            detections += unshift_dets\n",
    "            \n",
    "        for i in range(1, 5):\n",
    "            shift_img = np.empty(img.shape)\n",
    "            shift_img[:, 0:(full_size - shift_size), :] = np.copy(img[:, shift_size:full_size, :])\n",
    "            shift_img[:, (full_size - shift_size):full_size, :] = np.copy(img[:, 0:shift_size, :])\n",
    "            \n",
    "            blocks = np.empty((4, block_size, block_size, 3))\n",
    "            blocks[0, :, :, :] = shift_img[0 : block_size, 0 : block_size, :]\n",
    "            blocks[1, :, :, :] = shift_img[0 : block_size, block_size : full_size, :]\n",
    "            blocks[2, :, :, :] = shift_img[block_size : full_size, 0 : block_size, :]\n",
    "            blocks[3, :, :, :] = shift_img[block_size : full_size, block_size : full_size, :]\n",
    "\n",
    "            valid_indices = []\n",
    "            testing_blocks = np.empty((4, VEC_SIZE))\n",
    "            for i in range(4):\n",
    "                block = cv2.resize(blocks[i, :, :], (256, 256), interpolation = cv2.INTER_NEAREST)\n",
    "                green_locs = np.where((block[:, :, 0] == 0) & (block[:, :, 1] != 0) & (block[:, :, 2] != 0))[0]\n",
    "                block = np.reshape(block, VEC_SIZE)\n",
    "                if (block.mean() > 15) and (green_locs.size < block.size / 3):\n",
    "                    valid_indices = np.append(valid_indices, i)\n",
    "                    testing_blocks[i, :] = block\n",
    "\n",
    "            current_dets = np.zeros(4)\n",
    "\n",
    "            if len(valid_indices) != 0:\n",
    "                testing_blocks = testing_blocks[np.array(valid_indices).astype(int), :]\n",
    "                transform_test = pca.transform(testing_blocks)\n",
    "                split_pred = svm.predict(transform_test)\n",
    "                index = 0\n",
    "                for i in range(4):\n",
    "                    if i in valid_indices:\n",
    "                        current_dets[i] = split_pred[index]\n",
    "                        index += 1\n",
    "\n",
    "            block_detections = np.zeros((full_size, full_size))\n",
    "\n",
    "            block_detections[0 : block_size, 0 : block_size] = np.full((block_size, block_size), current_dets[0]) \\\n",
    "                        + break_blocks(blocks[0, :, :, :])\n",
    "            block_detections[0 : block_size, block_size : full_size] = np.full((block_size, block_size), current_dets[1]) \\\n",
    "                        + break_blocks(blocks[1, :, :, :])\n",
    "            block_detections[block_size : full_size, 0 : block_size] = np.full((block_size, block_size), current_dets[2]) \\\n",
    "                        + break_blocks(blocks[2, :, :, :])\n",
    "            block_detections[block_size : full_size, block_size : full_size] = np.full((block_size, block_size), current_dets[3]) \\\n",
    "                        + break_blocks(blocks[3, :, :, :])\n",
    "            \n",
    "            unshift_dets = np.empty((full_size, full_size))\n",
    "            unshift_dets[:, 0:(i * shift_size)] = np.copy(block_detections[:, (full_size - (shift_size * i)):full_size])\n",
    "            unshift_dets[:, (i * shift_size):full_size] = np.copy(block_detections[:, 0:(full_size - (shift_size * i))])\n",
    "            \n",
    "            detections += unshift_dets\n",
    "        \n",
    "        return detections\n",
    "    else:\n",
    "        return np.zeros((full_size, full_size))\n",
    "\n",
    "    \n",
    "def classify(img_loc):\n",
    "    \"\"\"\n",
    "    contrast and classify new test image with divide/shift/conquer pipeline\n",
    "    :param img_loc: path to 2048x2048 image\n",
    "    :return: detections, nd-array shape 2048x2048 - contains accumulated binary labels at each pixel\n",
    "    \"\"\"\n",
    "    \n",
    "    img = contrast_resize(img_loc, (2048, 2048))\n",
    "    img = img[:, :, :3]\n",
    "\n",
    "    detections = break_blocks(img)\n",
    "    detections = np.rint(detections)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 9));\n",
    "    ax1.imshow(img);\n",
    "    ax1.title.set_text('Original Test Image');\n",
    "    ax1.axis('off');\n",
    "    \n",
    "    normalized = detections / np.linalg.norm(detections)\n",
    "    superimposed = np.empty(img.shape)\n",
    "    for i in range(3):\n",
    "        superimposed[:, :, i] = normalized * img[:, :, i]\n",
    "        \n",
    "    ax2.imshow(superimposed);\n",
    "    ax2.title.set_text('Classifier Decisions\\nSuperimposed on Image');\n",
    "    ax2.axis('off');\n",
    "    \n",
    "    fig.savefig('FIGURES/detection_maps_' + str(img_loc.split('/')[-1].split('.')[0].split('-')[0]) + '.png')\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "for img in TEST_IMGS:\n",
    "    detections = classify(os.path.join(TEST_DIR, str(img) + '-Stitched.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform training w/ pca and eigenimaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposed test set to 41 components\n",
      "\n",
      "Optimal SVM parameter values:\n",
      "C: 26.366508987303554\n",
      "kernel: rbf\n",
      "gamma: 5.4555947811685143e-05\n",
      "degree: 2\n",
      "coef0: 0 \n",
      "\n",
      "Calculating metrics...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.77      0.87        44\n",
      "           1       0.78      1.00      0.88        36\n",
      "\n",
      "    accuracy                           0.88        80\n",
      "   macro avg       0.89      0.89      0.87        80\n",
      "weighted avg       0.90      0.88      0.87        80\n",
      "\n",
      "\n",
      "Average cross-validate score:  0.8745421245421245\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, tags, test_size = 0.2)\n",
    "\n",
    "# perform pca transform\n",
    "pca = PCA(n_components = 0.9, svd_solver = 'full').fit(X_train)\n",
    "print('Decomposed test set to', pca.components_.shape[0], 'components')\n",
    "X_train_PCA = pca.transform(X_train)\n",
    "\n",
    "# apply pca transform to x_test\n",
    "X_test_PCA = pca.transform(X_test)\n",
    "\n",
    "# fit classifier to data\n",
    "C_RANGE = np.logspace(-3, 3, 20)\n",
    "G_RANGE = np.logspace(-9, 1, 20)\n",
    "param_grid = { 'C': C_RANGE,\n",
    "               'gamma': ['scale', 'auto'],\n",
    "               'gamma': G_RANGE,\n",
    "               'degree': [2, 3, 4],\n",
    "               'kernel': ['rbf', 'poly'], \n",
    "               'coef0': [0, 1],\n",
    "               'class_weight': ['balanced'] }\n",
    "\n",
    "svm = GridSearchCV(SVC(), param_grid, n_jobs = -1, verbose = 0)\n",
    "svm.fit(X_train_PCA, y_train)\n",
    "\n",
    "Copt = svm.best_params_['C'] # svm cost parameter\n",
    "Kopt = svm.best_params_['kernel'] # kernel function\n",
    "Gopt = svm.best_params_['gamma'] # gamma of RBF kernel\n",
    "Dopt = svm.best_params_['degree'] # degree of polynomial kernel\n",
    "Zopt = svm.best_params_['coef0'] # independent term in poly kernel\n",
    "\n",
    "print('\\nOptimal SVM parameter values:')\n",
    "print('C:', Copt)\n",
    "print('kernel:', Kopt)\n",
    "print('gamma:', Gopt)\n",
    "print('degree:', Dopt)\n",
    "print('coef0:', Zopt, '\\n')\n",
    "\n",
    "# generate report\n",
    "print('Calculating metrics...')\n",
    "y_pred = svm.predict(X_test_PCA)\n",
    "print(classification_report(y_test, y_pred))\n",
    "scores = cross_val_score(svm, X_test_PCA, y_test, cv = 6)\n",
    "print('\\nAverage cross-validate score: ', scores.mean())\n",
    "\n",
    "# dump model data for safe-keeping\n",
    "pickle.dump(svm, open('MODELS/svm_model_' + datetime.now().strftime(\"%d:%m:%Y_%H:%M:%S\") + '.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save optimal model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svm, open('MODELS/svm_model_opt.sav', 'wb'))\n",
    "pickle.dump(pca, open('MODELS/pca_model_opt.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load optimal model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = pickle.load(open('MODELS/svm_model_opt.sav', 'rb'))\n",
    "pca = pickle.load(open('MODELS/pca_model_opt.sav', 'rb'))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, tags, test_size = 0.2)\n",
    "X_test_PCA = pca.transform(X_test)\n",
    "y_pred = svm.predict(X_test_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contrast and resize new training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, d, f in os.walk(TRAIN_DIR + 'PROCESS/'):\n",
    "    for file in f:\n",
    "        num = int(file.split('.')[0])\n",
    "        img = contrast_resize(os.path.join(TRAIN_DIR, 'PROCESS/' + str(num) + '.png'), (256, 256))\n",
    "        plt.imsave(os.path.join(TRAIN_DIR, 'ONE/' + str(num) + '.png'), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display contrasted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(7, 7, figsize = (15, 20));\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(data[i, :].reshape(VEC_DIM))\n",
    "    ax.title.set_text('VALUE ' + str(int(tags[i])))\n",
    "    ax.title.set_fontsize(14)\n",
    "    ax.axis('off')\n",
    "fig.savefig('FIGURES/display_dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize results w/ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 6, figsize = (15, 20));\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test[i - 1, :].reshape(VEC_DIM))\n",
    "    ax.title.set_text('VALUE ' + str(int(y_test[i - 1])) + \"\\n PREDICTED \" + str(int(y_pred[i - 1])))\n",
    "    ax.title.set_fontsize(14)\n",
    "    ax.axis('off')\n",
    "plt.savefig('FIGURES/sample_outputs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca with uncontrasted data - fewer components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_MIN = PCA(n_components = pca.components_.shape[0]).fit(data)\n",
    "plt.figure(figsize = (10, 7));\n",
    "plt.plot(pca_MIN.explained_variance_ratio_.cumsum(), color = 'seagreen');\n",
    "plt.xlabel('Number of Principal Components');\n",
    "plt.ylabel('Explained Variance Ratio');\n",
    "plt.title('Explained Variance Ratio of\\nPrincipal Components in a Contrasted Dataset');\n",
    "plt.savefig('FIGURES/pca_variance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_indices = svm.best_estimator_.support_\n",
    "fig, axes = plt.subplots(5, 4, figsize = (18, 22))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < sv_indices.size:\n",
    "        ax.imshow(pca.inverse_transform(svm.best_estimator_.support_vectors_[i, :]).reshape((VEC_DIM)).astype(float).clip(0, 1))\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "fig.savefig('FIGURES/support_vectors.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize eigenfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 4, figsize = (18, 25))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < pca.components_.shape[0]:\n",
    "        ax.title.set_text(str(pca.singular_values_[i]))\n",
    "        ax.imshow((pca.components_[i, :].reshape((VEC_DIM)) * 255).astype(float).clip(0, 1))\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "fig.savefig('FIGURES/eigenfaces.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance of training space parameter vs num support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G_RANGE = np.logspace(-9, 1, 20)\n",
    "\n",
    "NUM_SVS = []\n",
    "\n",
    "data_PCA = pca.transform(data)\n",
    "\n",
    "for G in G_RANGE:\n",
    "    svm = SVC(C = 1, kernel = 'rbf', gamma = G, class_weight = 'balanced')\n",
    "    svm.fit(data_PCA, tags)\n",
    "    NUM_SVS.append(svm.support_vectors_.shape[0])\n",
    "    \n",
    "plt.figure(figsize = (10, 7));\n",
    "plt.plot(C_RANGE, NUM_SVS, color = 'lightcoral');\n",
    "plt.xscale('log');\n",
    "plt.xlabel('G');\n",
    "plt.ylabel('Support Vectors');\n",
    "plt.title('Gamma Value (Variance of Training Space)\\nvs. Number of Support Vectors')\n",
    "plt.savefig('FIGURES/gamma_sv_tradeoff.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma vs number of support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_RANGE = np.logspace(-2, 4, 20)\n",
    "\n",
    "NUM_SVS = []\n",
    "\n",
    "data_PCA = pca.transform(data)\n",
    "\n",
    "for C in C_RANGE:\n",
    "    svm = SVC(C = C, kernel = 'rbf', gamma = 'scale', class_weight = 'balanced')\n",
    "    svm.fit(data_PCA, tags)\n",
    "    NUM_SVS.append(svm.support_vectors_.shape[0])\n",
    "    \n",
    "plt.figure(figsize = (10, 7));\n",
    "plt.plot(C_RANGE, NUM_SVS, color = 'darkorchid');\n",
    "plt.xscale('log');\n",
    "plt.xlabel('C');\n",
    "plt.ylabel('Support Vectors');\n",
    "plt.title('Cost Value (L2 Regularization Parameter)\\nvs. Number of Support Vectors')\n",
    "plt.savefig('FIGURES/cost_sv_tradeoff.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple pca example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 6))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection = '3d')\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "n = 100\n",
    "\n",
    "xyz = np.empty((n, 3))\n",
    "\n",
    "## generate random 3D data\n",
    "xyz[:, 0] = np.linspace(0, 1, 100)\n",
    "xyz[:, 1] = np.sin(9 * xyz[:, 0]) + np.sqrt(1 / 3.0) * np.random.randn(n)\n",
    "xyz[:, 2] = np.random.rand(n)\n",
    "\n",
    "## define binary classes - green triangle, pink circle\n",
    "for i in range(xyz.shape[0]):\n",
    "    if xyz[i, 1] < 0.5:\n",
    "        ax1.scatter(xyz[i, 0], xyz[i, 1], xyz[i, 2], marker = '^', color = 'green')\n",
    "    else:\n",
    "        ax1.scatter(xyz[i, 0], xyz[i, 1], xyz[i, 2], marker = 'o', color = 'hotpink')\n",
    "\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "ax1.set_title('Data before PCA (3D)')\n",
    "\n",
    "for label in (ax1.get_xticklabels() + ax1.get_yticklabels() + ax1.get_zticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "\n",
    "## reduce dimension from 3D to 2D\n",
    "pca_xyz = PCA(n_components = 2).fit(xyz)\n",
    "xyz_d = pca_xyz.transform(xyz)\n",
    "\n",
    "for i in range(xyz.shape[0]):\n",
    "    if xyz[i, 1] < 0.5:\n",
    "        ax2.scatter(xyz_d[i, 0], xyz_d[i, 1], marker = '^', color = 'green')\n",
    "    else:\n",
    "        ax2.scatter(xyz_d[i, 0], xyz_d[i, 1], marker = 'o', color = 'hotpink')\n",
    "\n",
    "ax2.set_xlabel('PC 1')\n",
    "ax2.set_ylabel('PC 2')\n",
    "ax2.set_title('Data after PCA (2D)')\n",
    "\n",
    "for label in (ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "\n",
    "fig.savefig('FIGURES/pca_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding coordinates with spice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the kernels\n",
    "KERNELS = []\n",
    "for r, d, f in os.walk(KERNEL_DIR):\n",
    "    for file in f:\n",
    "        if not fnmatch.fnmatch(file, '*Zone.Identifier'):\n",
    "            KERNELS.append(file)\n",
    "\n",
    "spice.kclear()\n",
    "for k in KERNELS:\n",
    "    spice.furnsh(os.path.join(KERNEL_DIR, k))\n",
    "\n",
    "## read image metadata\n",
    "with open(os.path.join(TEST_DIR, TEST_NAME + '-Metadata.json'), 'r') as f:\n",
    "    img_json = json.load(f)\n",
    "    image = img_json['FILE_NAME']\n",
    "    image_time = img_json['START_TIME']\n",
    "et = spice.str2et(image_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find lat/long of camera location of jupiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get position of juno relative to jupiter at elapsed time\n",
    "pos, lt = spice.spkpos('JUNO', et, 'IAU_JUPITER', 'NONE', 'JUPITER')\n",
    "\n",
    "## rotation matrix from juno spacecraft to 'cube' modeling camera - from juno_v12.tf\n",
    "JUNO_TO_CUBE = np.matrix([[-0.0059163, -0.0142817, -0.9998805], \n",
    "                          [ 0.0023828, -0.9998954,  0.0142678], \n",
    "                          [-0.9999797, -0.0022981,  0.0059497]])\n",
    "\n",
    "## rotation matrix from 'cube' to camera (aberration correction) - from juno_v12.tf\n",
    "CUBE_TO_CAM = (R.from_euler('zyx', [0.69, -0.469,  0.583])).as_matrix()\n",
    "\n",
    "## combine rotation matrices - application order JUNO_TO_CUBE then CUBE_TO_CAM\n",
    "rot_matrix = CUBE_TO_CAM * JUNO_TO_CUBE\n",
    "\n",
    "## apply rotations to juno position to get position camera points to\n",
    "pos = rot_matrix * pos.reshape(-1, 1)\n",
    "\n",
    "## calculate planetocentric lat/long coordinates in radians\n",
    "_, long, lat = spice.reclat(np.ravel(pos))\n",
    "\n",
    "## convert coordinates to degrees\n",
    "long, lat = np.array([long, lat]) * 180 / math.pi\n",
    "\n",
    "## get orientation of camera\n",
    "orient = spice.pxform('IAU_JUPITER', 'JUNO_JUNOCAM', et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert surface raster to lat/long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = np.load(os.path.join(TEST_DIR, TEST_NAME + '-Raster.npy'))\n",
    "coords = np.empty((2048, 2048, 2))\n",
    "for i in range(2048):\n",
    "    for j in range(2048):\n",
    "        if abs(np.mean(raster[i, j, :])) != 0:\n",
    "            _, long, lat = spice.reclat(raster[i, j, :])\n",
    "            coords[i, j, :] = np.array([long, lat]) * 180 / math.pi\n",
    "        else:\n",
    "            coords[i, j, :] = np.array([np.nan, np.nan]) # outside range\n",
    "np.save(os.path.join(TEST_DIR, TEST_NAME + '-Coords', coords))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "ax2.imshow(raster)\n",
    "ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save detections in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_indices = np.argwhere(threshold == 1)\n",
    "det_coords = np.empty((det_indices.shape[0], 2))\n",
    "for i in range(det_indices.shape[0]):\n",
    "    det_coords[i, :] = coords[det_indices[i, 0], det_indices[i, 1]]\n",
    "np.save('LOGS/' + TEST_NAME + '_' + image_time, det_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrain model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ** detections should be certified to be correct by human user ** ##\n",
    "fig, axes = plt.subplots(6, 6, figsize = (18, 20))\n",
    "green_indices = []\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    block = retrain_blocks[i, :].reshape((VEC_DIM)) * 255\n",
    "    if len(np.where((block[:, :, 0] == 0) & (block[:, :, 1] != 0) & (block[:, :, 2] != 0))[0]) == 0:\n",
    "        ax.imshow(block / 255)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "        green_indices.append(i)\n",
    "retrain_blocks = np.delete(retrain_blocks, green_indices, axis = 0)\n",
    "\n",
    "curr = TRAIN_SIZE + 1\n",
    "for i in range(retrain_blocks.shape[0]):\n",
    "    plt.imsave(str(curr) + '.png', retrain_blocks[i, :].reshape((VEC_DIM)))\n",
    "    curr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
